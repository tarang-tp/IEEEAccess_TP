{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# üìß Email Spam Filtering - Tarang Pande\n",
        "\n",
        "## A Calibrated Dual-Channel Ensemble Approach for Fast Enterprise-Scale Email Spam Detection\n",
        "\n",
        "---\n",
        "\n",
        "**Key Features:**\n",
        "- ‚úÖ Dual-channel TF-IDF (8k word + 30k char + 2 numeric)\n",
        "- ‚úÖ Ensemble: Linear SVM + Complement Na√Øve Bayes + Gradient Boosting\n",
        "- ‚úÖ Soft voting with weights [3, 1, 1]\n",
        "- ‚úÖ Platt calibration (5-fold CV)\n",
        "- ‚úÖ 20 stratified splits for robustness\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìë Table of Contents\n",
        "\n",
        "1. [Setup & Imports](#1-setup-imports)\n",
        "2. [Data Loading](#2-data-loading)\n",
        "3. [Pre-processing (Section 4.1)](#3-preprocessing)\n",
        "4. [Dual-Channel Feature Extraction (Section 4.2)](#4-feature-extraction)\n",
        "5. [Base Learners (Section 4.3)](#5-base-learners)\n",
        "6. [Ensemble Construction (Section 4.4)](#6-ensemble-construction)\n",
        "7. [Training & Evaluation (Section 3)](#7-training-evaluation)\n",
        "8. [Results (Section 5.1)](#8-results)\n",
        "9. [Ablation Study (Section 5.2)](#9-ablation-study)\n",
        "10. [Model Saving & Analysis](#10-model-saving)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='1-setup-imports'></a>\n",
        "## 1Ô∏è‚É£ Setup & Imports\n",
        "\n",
        "Install required packages if needed:\n",
        "```bash\n",
        "pip install scikit-learn pandas numpy nltk beautifulsoup4 scipy matplotlib seaborn joblib\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "EMAIL SPAM FILTERING\n",
            "======================================================================\n",
            "\n",
            "üîß Configuration:\n",
            "   ‚Ä¢ Parallel processing: 8 cores\n",
            "   ‚Ä¢ 20-fold evaluation\n",
            "   ‚Ä¢ Dual-channel TF-IDF (38k features)\n",
            "   ‚Ä¢ Ensemble: SVM + ComplementNB + GradientBoosting\n",
            "\n",
            "‚è±Ô∏è  Expected runtime: ~15-30 minutes (with parallel processing)\n",
            "======================================================================\n",
            "‚úÖ All imports successful!\n",
            "‚úÖ Configuration: 20 splits, random_state=42\n"
          ]
        }
      ],
      "source": [
        "# Core imports\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import warnings\n",
        "import os\n",
        "import time\n",
        "from joblib import Parallel, delayed  # For parallel processing\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# NLP libraries\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from bs4 import BeautifulSoup\n",
        "import unicodedata\n",
        "\n",
        "# Scikit-learn\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier\n",
        "from sklearn.naive_bayes import ComplementNB\n",
        "from sklearn.calibration import CalibratedClassifierCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, f1_score, roc_auc_score,\n",
        "    average_precision_score, brier_score_loss,\n",
        "    log_loss, matthews_corrcoef\n",
        ")\n",
        "import scipy.sparse as sp\n",
        "import joblib\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('stopwords', quiet=True)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"EMAIL SPAM FILTERING\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nüîß Configuration:\")\n",
        "print(\"   ‚Ä¢ Parallel processing: 8 cores\")\n",
        "print(\"   ‚Ä¢ 20-fold evaluation\")\n",
        "print(\"   ‚Ä¢ Dual-channel TF-IDF (38k features)\")\n",
        "print(\"   ‚Ä¢ Ensemble: SVM + ComplementNB + GradientBoosting\")\n",
        "print(\"\\n‚è±Ô∏è  Expected runtime: ~15-30 minutes (with parallel processing)\")\n",
        "print(\"=\"*70)\n",
        "RANDOM_STATE = 42\n",
        "SPLITS = 20  # Paper uses 20 splits (Section 3)\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n",
        "print(f\"‚úÖ Configuration: {SPLITS} splits, random_state={RANDOM_STATE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "os.makedirs('results', exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='2-data-loading'></a>\n",
        "## 2Ô∏è‚É£ Data Loading\n",
        "\n",
        "**From Paper (Section 3):**\n",
        "> \"For this study, a subset of the Enron Corpus dataset was utilized. We obtained a mix Spam/Ham dataset based on the Enron Corpus from **MWiechmann** We randomly chose approximately 34,000 emails... Following these cleaning procedures, the dataset comprised **30,462 unique messages**.\"\n",
        "\n",
        "**Expected Dataset Statistics:**\n",
        "- Total emails: ~30,462\n",
        "- Spam: ~14,552 (47.8%)\n",
        "- Ham: ~15,910 (52.2%)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "LOADING ENRON CORPUS DATASET\n",
            "======================================================================\n",
            "\n",
            "üìä Dataset Statistics:\n",
            "   Total emails: 33,665\n",
            "   Spam emails:  17,120 (50.9%)\n",
            "   Ham emails:   16,545 (49.1%)\n",
            "\n",
            "üìß Sample messages:\n",
            "                                                text  label\n",
            "0                       christmas tree farm pictures      0\n",
            "1  vastar resources , inc . gary , production fro...      0\n",
            "2  calpine daily gas nomination - calpine daily g...      0\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"LOADING ENRON CORPUS DATASET\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('enron_spam_data.csv')\n",
        "\n",
        "# Create combined text field (Subject + Message)\n",
        "df['text'] = (df['Subject'].fillna('') + ' ' + df['Message'].fillna('')).str.strip()\n",
        "df = df[df['text'].str.len() > 0].reset_index(drop=True)\n",
        "\n",
        "# Binary label\n",
        "df['label'] = (dfs['Spam/Ham'] == 'spam').astype(int)\n",
        "\n",
        "# Display statistics\n",
        "print(f\"\\nüìä Dataset Statistics:\")\n",
        "print(f\"   Total emails: {len(df):,}\")\n",
        "print(f\"   Spam emails:  {(df['label']==1).sum():,} ({df['label'].mean()*100:.1f}%)\")\n",
        "print(f\"   Ham emails:   {(df['label']==0).sum():,} ({(1-df['label'].mean())*100:.1f}%)\")\n",
        "\n",
        "# Show sample\n",
        "print(f\"\\nüìß Sample messages:\")\n",
        "print(df[['text', 'label']].head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "DEDUPLICATING DATASET (preserving class balance)\n",
            "======================================================================\n",
            "\n",
            "Before: 33,665 rows\n",
            "   Spam: 17,120\n",
            "   Ham:  16,545\n",
            "\n",
            "After dedup:\n",
            "   Unique spam: 14,552\n",
            "   Unique ham:  15,910\n",
            "\n",
            "Final dataset: 30,462 rows\n",
            "   Spam: 14,552 (47.8%)\n",
            "   Ham:  15,910 (52.2%)\n"
          ]
        }
      ],
      "source": [
        "# DEDUPLICATE WITHIN EACH CLASS\n",
        "print(\"=\"*70)\n",
        "print(\"DEDUPLICATING DATASET (preserving class balance)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(f\"\\nBefore: {len(df):,} rows\")\n",
        "print(f\"   Spam: {(df['label']==1).sum():,}\")\n",
        "print(f\"   Ham:  {(df['label']==0).sum():,}\")\n",
        "\n",
        "# Deduplicate separately for each class\n",
        "df_spam = df[df['label'] == 1].drop_duplicates(subset=['text'], keep='first')\n",
        "df_ham = df[df['label'] == 0].drop_duplicates(subset=['text'], keep='first')\n",
        "\n",
        "print(f\"\\nAfter dedup:\")\n",
        "print(f\"   Unique spam: {len(df_spam):,}\")\n",
        "print(f\"   Unique ham:  {len(df_ham):,}\")\n",
        "\n",
        "# Combine back\n",
        "df_dedup = pd.concat([df_spam, df_ham], ignore_index=True)\n",
        "\n",
        "# Shuffle\n",
        "df_dedup = df_dedup.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(f\"\\nFinal dataset: {len(df_dedup):,} rows\")\n",
        "print(f\"   Spam: {(df_dedup['label']==1).sum():,} ({df_dedup['label'].mean()*100:.1f}%)\")\n",
        "print(f\"   Ham:  {(df_dedup['label']==0).sum():,} ({(1-df_dedup['label'].mean())*100:.1f}%)\")\n",
        "\n",
        "# Replace df\n",
        "df = df_dedup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='3-preprocessing'></a>\n",
        "## 3Ô∏è‚É£ Pre-processing (Paper Section 4.1)\n",
        "\n",
        "**From Paper:**\n",
        "> \"The text pre-processing pipeline involved several crucial stages:\n",
        "> - Unicode NFC normalization\n",
        "> - HTML stripping (BeautifulSoup4)\n",
        "> - URL masking to `<URL>`\n",
        "> - Lowercasing\n",
        "> - Regex cleaning\n",
        "> - Stopword removal  \n",
        "> - Porter stemming\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "PRE-PROCESSING TEXT\n",
            "======================================================================\n",
            "\n",
            "‚úÖ Preprocessing complete!\n",
            "   Emails after preprocessing: 30,452\n",
            "\n",
            "üìß Example:\n",
            "\n",
            "BEFORE: pay les for acrobat 6 professional pc weekly : review results after a thorough comparison of the various retailers , the best offer is : wlndows x ' p - 50 doiiar ( 150 doiiar less )\n",
            "do you want this ...\n",
            "\n",
            "AFTER:  pay le acrobat profession pc weekli review result thorough comparison variou retail best offer wlndow x p doiiar doiiar less want stuff conduct increaseconfess poach admixcarcinoma suprem anglinglowbo...\n"
          ]
        }
      ],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"\n",
        "    Pre-process email text as described in Paper Section 4.1.\n",
        "    \n",
        "    Steps:\n",
        "    1. Unicode NFC normalization\n",
        "    2. HTML stripping\n",
        "    3. URL masking to <URL>\n",
        "    4. Lowercasing\n",
        "    5. Remove punctuation\n",
        "    6. Remove stopwords\n",
        "    7. Porter stemming\n",
        "    \n",
        "    Args:\n",
        "        text (str): Raw email text\n",
        "        \n",
        "    Returns:\n",
        "        str: Processed text\n",
        "    \"\"\"\n",
        "    if pd.isna(text):\n",
        "        return \"\"\n",
        "    \n",
        "    # 1. Unicode normalization\n",
        "    text = unicodedata.normalize('NFC', str(text))\n",
        "    \n",
        "    # 2. HTML stripping\n",
        "    text = BeautifulSoup(text, 'html.parser').get_text()\n",
        "    \n",
        "    # 3. URL masking (PRESERVE <URL> token for counting)\n",
        "    text = re.sub(r'http\\S+|www\\.\\S+', '<URL>', text)\n",
        "    \n",
        "    # 4. Lowercase\n",
        "    text = text.lower()\n",
        "    \n",
        "    # 5. Remove punctuation (preserve <URL>)\n",
        "    text = re.sub(r'[^a-z\\s<>]', ' ', text)\n",
        "    \n",
        "    # 6. Tokenize and remove stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [w for w in text.split() if w and (w == '<url>' or w not in stop_words)]\n",
        "    \n",
        "    # 7. Stemming\n",
        "    stemmer = PorterStemmer()\n",
        "    tokens = [stemmer.stem(w) if w != '<url>' else w for w in tokens]\n",
        "    \n",
        "    return ' '.join(tokens)\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"PRE-PROCESSING TEXT\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Apply preprocessing\n",
        "df['processed'] = df['text'].apply(preprocess_text)\n",
        "df = df[df['processed'].str.len() > 0].reset_index(drop=True)\n",
        "\n",
        "print(f\"\\n‚úÖ Preprocessing complete!\")\n",
        "print(f\"   Emails after preprocessing: {len(df):,}\")\n",
        "\n",
        "# Show before/after example\n",
        "print(f\"\\nüìß Example:\")\n",
        "idx = 100\n",
        "print(f\"\\nBEFORE: {df['text'].iloc[idx][:200]}...\")\n",
        "print(f\"\\nAFTER:  {df['processed'].iloc[idx][:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='4-feature-extraction'></a>\n",
        "## 4Ô∏è‚É£ Dual-Channel Feature Extraction (Paper Section 4.2)\n",
        "\n",
        "**From Paper:**\n",
        "> \"For the word channel, 1-2-gram TF-IDF features were generated... the word vocabulary size was fixed at **8k**.\"\n",
        ">\n",
        "> \"The character channel focused on 3-5-gram TF-IDF features... **30k** delivering the most favorable trade-off.\"\n",
        ">\n",
        "> \"The final concatenated matrix comprised **38,000 textual features** along with **2 additional numeric columns** (log-scaled message length and URL token count).\"\n",
        "\n",
        "### Architecture:\n",
        "\n",
        "```\n",
        "[Word Channel]     ‚Üí 8,000 features (1-2 grams)\n",
        "[Char Channel]     ‚Üí 30,000 features (3-5 grams)\n",
        "[Numeric Features] ‚Üí 2 features (log-length, URL count)\n",
        "‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ\n",
        "TOTAL:             ‚Üí 38,002 features\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "DUAL-CHANNEL FEATURE EXTRACTION (Paper Section 4.2)\n",
            "======================================================================\n",
            "\n",
            "[1/4] üìù Word Channel (1-2 grams)...\n",
            "      Paper: 'word vocabulary size was fixed at 8k'\n",
            "      ‚úÖ Word features: (30452, 8000)\n",
            "\n",
            "[2/4] üî§ Character Channel (3-5 grams)...\n",
            "      Paper: '30k delivering most favorable trade-off'\n",
            "      ‚úÖ Character features: (30452, 30000)\n",
            "\n",
            "[3/4] üî¢ Numeric Meta-features...\n",
            "      Paper: 'log-scaled message length and URL token count'\n",
            "      ‚úÖ Numeric features: (30452, 2)\n",
            "         - Log message length (mean: 4.26)\n",
            "         - URL count (mean: 0.00)\n",
            "\n",
            "[4/4] üîó Concatenating all features...\n",
            "\n",
            "======================================================================\n",
            "‚úÖ FINAL FEATURE MATRIX\n",
            "======================================================================\n",
            "Shape:           (30452, 38002)\n",
            "\n",
            "Sparsity:        0.9672\n",
            "\n",
            "Memory (MB):     289.7\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"DUAL-CHANNEL FEATURE EXTRACTION (Paper Section 4.2)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ============================================================================\n",
        "# WORD CHANNEL: 1-2 gram TF-IDF, 8k features\n",
        "# ============================================================================\n",
        "print(\"\\n[1/4] üìù Word Channel (1-2 grams)...\")\n",
        "print(\"      Paper: 'word vocabulary size was fixed at 8k'\")\n",
        "\n",
        "word_vectorizer = TfidfVectorizer(\n",
        "    max_features=8000,      # Paper Section 4.2\n",
        "    min_df=2,\n",
        "    max_df=0.95,\n",
        "    ngram_range=(1, 2),     # 1-2 grams\n",
        "    sublinear_tf=True,      # Sub-linear TF scaling\n",
        "    analyzer='word',\n",
        "    smooth_idf=True         # IDF smoothing (1+df)\n",
        ")\n",
        "X_word = word_vectorizer.fit_transform(df['processed'])\n",
        "print(f\"      ‚úÖ Word features: {X_word.shape}\")\n",
        "\n",
        "# ============================================================================\n",
        "# CHARACTER CHANNEL: 3-5 gram TF-IDF, 30k features\n",
        "# ============================================================================\n",
        "print(\"\\n[2/4] üî§ Character Channel (3-5 grams)...\")\n",
        "print(\"      Paper: '30k delivering most favorable trade-off'\")\n",
        "\n",
        "char_vectorizer = TfidfVectorizer(\n",
        "    max_features=30000,     # Paper Section 4.2\n",
        "    min_df=5,               # Paper: 'min_df parameter of 5'\n",
        "    ngram_range=(3, 5),     # 3-5 character n-grams\n",
        "    sublinear_tf=True,\n",
        "    analyzer='char',\n",
        "    smooth_idf=True\n",
        ")\n",
        "X_char = char_vectorizer.fit_transform(df['processed'])\n",
        "print(f\"      ‚úÖ Character features: {X_char.shape}\")\n",
        "\n",
        "# ============================================================================\n",
        "# NUMERIC META-FEATURES: log(length) and URL count\n",
        "# ============================================================================\n",
        "print(\"\\n[3/4] üî¢ Numeric Meta-features...\")\n",
        "print(\"      Paper: 'log-scaled message length and URL token count'\")\n",
        "\n",
        "# Feature 1: Log-scaled message length (in tokens)\n",
        "msg_lengths = df['processed'].str.split().str.len().values\n",
        "log_msg_length = np.log1p(msg_lengths).reshape(-1, 1)\n",
        "\n",
        "# Feature 2: URL token count\n",
        "url_counts = df['processed'].str.count('<url>').values.reshape(-1, 1)\n",
        "\n",
        "# Convert to sparse matrix\n",
        "X_numeric = sp.csr_matrix(np.hstack([log_msg_length, url_counts]))\n",
        "print(f\"      ‚úÖ Numeric features: {X_numeric.shape}\")\n",
        "print(f\"         - Log message length (mean: {log_msg_length.mean():.2f})\")\n",
        "print(f\"         - URL count (mean: {url_counts.mean():.2f})\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONCATENATE ALL CHANNELS\n",
        "# ============================================================================\n",
        "print(\"\\n[4/4] üîó Concatenating all features...\")\n",
        "\n",
        "X = sp.hstack([X_word, X_char, X_numeric])\n",
        "y = df['label'].values\n",
        "\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"‚úÖ FINAL FEATURE MATRIX\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"Shape:           {X.shape}\")\n",
        "print(f\"\\nSparsity:        {1 - X.nnz / (X.shape[0] * X.shape[1]):.4f}\")\n",
        "print(f\"\\nMemory (MB):     {X.data.nbytes / 1024 / 1024:.1f}\")\n",
        "print(f\"{'='*70}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='5-base-learners'></a>\n",
        "## 5Ô∏è‚É£ Base Learners (Paper Section 4.3)\n",
        "\n",
        "**From Paper:**\n",
        "> \"The ensemble model integrates three distinct base learners:\n",
        "> 1. **Linear SVM** - C=1.0, hinge loss, dual formulation; 5-fold Platt scaling\n",
        "> 2. **Complement Na√Øve Bayes** - alpha=1.0; class priors learned from data\n",
        "> 3. **Gradient Boosting** - n_estimators=100, learning_rate=0.1, depth=3, subsample=0.8\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "DEFINING BASE LEARNERS (Paper Section 4.3)\n",
            "======================================================================\n",
            "\n",
            "[1/3] ‚ö° Linear SVM with 5-fold Platt calibration...\n",
            "      Paper: 'C=1.0, hinge loss, dual formulation; 5-fold Platt scaling'\n",
            "      ‚úÖ SVM configured\n",
            "\n",
            "[2/3] üìä Complement Na√Øve Bayes...\n",
            "      Paper: 'alpha=1.0; class priors learned from data'\n",
            "      ‚úÖ Complement NB configured\n",
            "\n",
            "[3/3] üå≥ Gradient Boosting...\n",
            "      Paper: 'n_estimators=100, learning_rate=0.1, depth=3, subsample=0.8'\n",
            "      ‚úÖ Gradient Boosting configured\n",
            "\n",
            "======================================================================\n",
            "‚úÖ ALL BASE LEARNERS READY\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"DEFINING BASE LEARNERS (Paper Section 4.3)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# ============================================================================\n",
        "# 1. LINEAR SVM WITH PLATT CALIBRATION\n",
        "# ============================================================================\n",
        "print(\"\\n[1/3] ‚ö° Linear SVM with 5-fold Platt calibration...\")\n",
        "print(\"      Paper: 'C=1.0, hinge loss, dual formulation; 5-fold Platt scaling'\")\n",
        "\n",
        "svm_base = LinearSVC(\n",
        "    C=1.0,                  # Paper: 'C=1.0'\n",
        "    loss='hinge',           # Paper: 'hinge loss'\n",
        "    dual=True,              # Paper: 'dual formulation'\n",
        "    random_state=RANDOM_STATE,\n",
        "    max_iter=2000\n",
        ")\n",
        "\n",
        "svm_calibrated = CalibratedClassifierCV(\n",
        "    svm_base,\n",
        "    method='sigmoid',       # Paper: 'Platt scaling'\n",
        "    cv=5,                   # Paper: '5-fold'\n",
        "    n_jobs=-1\n",
        ")\n",
        "print(\"      ‚úÖ SVM configured\")\n",
        "\n",
        "# ============================================================================\n",
        "# 2. COMPLEMENT NA√èVE BAYES\n",
        "# ============================================================================\n",
        "print(\"\\n[2/3] üìä Complement Na√Øve Bayes...\")\n",
        "print(\"      Paper: 'alpha=1.0; class priors learned from data'\")\n",
        "\n",
        "nb = ComplementNB(\n",
        "    alpha=1.0               # Paper: 'alpha=1.0'\n",
        ")\n",
        "print(\"      ‚úÖ Complement NB configured\")\n",
        "\n",
        "# ============================================================================\n",
        "# 3. GRADIENT BOOSTING\n",
        "# ============================================================================\n",
        "print(\"\\n[3/3] üå≥ Gradient Boosting...\")\n",
        "print(\"      Paper: 'n_estimators=100, learning_rate=0.1, depth=3, subsample=0.8'\")\n",
        "\n",
        "gb = GradientBoostingClassifier(\n",
        "    n_estimators=100,       # Paper: 'n_estimators=100'\n",
        "    learning_rate=0.1,      # Paper: 'learning_rate=0.1'\n",
        "    max_depth=3,            # Paper: 'depth=3'\n",
        "    subsample=0.8,          # Paper: 'subsample=0.8'\n",
        "    random_state=RANDOM_STATE\n",
        ")\n",
        "print(\"      ‚úÖ Gradient Boosting configured\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ ALL BASE LEARNERS READY\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='6-ensemble-construction'></a>\n",
        "## 6Ô∏è‚É£ Ensemble Construction (Paper Section 4.4)\n",
        "\n",
        "**From Paper:**\n",
        "> \"The ensemble was constructed using a **soft-voting mechanism** implemented via VotingClassifier. The weights assigned to each base learner were **(3, 1, 1)** for Linear SVM, Complement Na√Øve Bayes, and Gradient Boosting, respectively. These weights were determined through a Bayesian optimization search using scikit-optimize, conducted over 40 iterations, with the objective of maximizing validation AUPRC and minimizing Brier Score.\"\n",
        "\n",
        "### Ensemble Configuration:\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ      SOFT VOTING ENSEMBLE           ‚îÇ\n",
        "‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§\n",
        "‚îÇ Linear SVM (weight=3)               ‚îÇ\n",
        "‚îÇ Complement NB (weight=1)            ‚îÇ\n",
        "‚îÇ Gradient Boosting (weight=1)        ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "         ‚Üì\n",
        "    Final Prediction\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ENSEMBLE CONSTRUCTION (Paper Section 4.4)\n",
            "======================================================================\n",
            "\n",
            "üì¶ Creating Voting Classifier...\n",
            "   Paper: 'soft-voting mechanism implemented via VotingClassifier'\n",
            "   Paper: 'weights (3, 1, 1) for Linear SVM, Complement NB, and GB'\n",
            "\n",
            "======================================================================\n",
            "‚úÖ ENSEMBLE CONFIGURED\n",
            "======================================================================\n",
            "\n",
            "üìä Configuration Summary:\n",
            "   Method:      Soft Voting\n",
            "   Weights:     [3, 1, 1]\n",
            "   Models:      SVM, ComplementNB, GradientBoosting\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"ENSEMBLE CONSTRUCTION (Paper Section 4.4)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "print(\"\\nüì¶ Creating Voting Classifier...\")\n",
        "print(\"   Paper: 'soft-voting mechanism implemented via VotingClassifier'\")\n",
        "print(\"   Paper: 'weights (3, 1, 1) for Linear SVM, Complement NB, and GB'\")\n",
        "\n",
        "ensemble = VotingClassifier(\n",
        "    estimators=[\n",
        "        ('svm', svm_calibrated),     \n",
        "        ('nb', nb),                   \n",
        "        ('gb', gb)                    \n",
        "    ],\n",
        "    voting='soft',                    # Paper: 'soft-voting'\n",
        "    weights=[3, 1, 1],                # Paper: 'weights (3, 1, 1)'\n",
        "    n_jobs=-1\n",
        ")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚úÖ ENSEMBLE CONFIGURED\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nüìä Configuration Summary:\")\n",
        "print(\"   Method:      Soft Voting\")\n",
        "print(\"   Weights:     [3, 1, 1]\")\n",
        "print(\"   Models:      SVM, ComplementNB, GradientBoosting\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='7-training-evaluation'></a>\n",
        "## 7Ô∏è‚É£ Training & Evaluation (Paper Section 3)\n",
        "\n",
        "**From Paper:**\n",
        "> \"For robust model evaluation, the dataset was subjected to **80/20 stratified splits, repeated 20 times** with different random seeds to account for variability and ensure the stability of the reported metrics.\"\n",
        "\n",
        "### Evaluation Protocol:\n",
        "\n",
        "```\n",
        "For split in [0...19]:\n",
        "    1. 80/20 stratified split (seed=split)\n",
        "    2. Train ensemble on training set\n",
        "    3. Evaluate on test set\n",
        "    4. Record metrics: Acc, F1, MCC, AUPRC, Brier\n",
        "    \n",
        "Final: Report mean ¬± std across all 20 splits\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "TRAINING WITH 20 STRATIFIED SPLITS (Paper Section 3)\n",
            "======================================================================\n",
            "\n",
            "Paper: 'dataset was subjected to 80/20 stratified splits,'\n",
            "       'repeated 20 times with different random seeds'\n",
            "\n",
            "======================================================================\n",
            "‚úÖ PARALLEL TRAINING COMPLETE!\n",
            "======================================================================\n",
            "Total time: 64.1 minutes (3845s)\n",
            "Average per split: 192.3s\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(f\"TRAINING WITH {SPLITS} STRATIFIED SPLITS (Paper Section 3)\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nPaper: 'dataset was subjected to 80/20 stratified splits,'\")\n",
        "print(\"       'repeated 20 times with different random seeds'\")\n",
        "\n",
        "from joblib import Parallel, delayed\n",
        "\n",
        "def train_single_split(split_idx, X, y, SPLITS):\n",
        "    \"\"\"Train ensemble on a single split - runs in parallel\"\"\"\n",
        "    from sklearn.model_selection import train_test_split\n",
        "    from sklearn.svm import LinearSVC\n",
        "    from sklearn.ensemble import GradientBoostingClassifier, VotingClassifier\n",
        "    from sklearn.naive_bayes import ComplementNB\n",
        "    from sklearn.calibration import CalibratedClassifierCV\n",
        "    from sklearn.metrics import (\n",
        "        accuracy_score, f1_score, matthews_corrcoef,\n",
        "        average_precision_score, brier_score_loss\n",
        "    )\n",
        "    import time\n",
        "    \n",
        "    # 80/20 stratified split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(\n",
        "        X, y,\n",
        "        test_size=0.2,\n",
        "        stratify=y,\n",
        "        random_state=split_idx\n",
        "    )\n",
        "    \n",
        "    # Recreate ensemble (CRITICAL for parallel execution)\n",
        "    svm = CalibratedClassifierCV(\n",
        "        LinearSVC(C=1.0, dual=True, max_iter=2000, random_state=42),\n",
        "        method='sigmoid',\n",
        "        cv=5\n",
        "    )\n",
        "    nb = ComplementNB(alpha=1.0)\n",
        "    gb = GradientBoostingClassifier(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=3,\n",
        "        subsample=0.8,\n",
        "        random_state=42\n",
        "    )\n",
        "    ensemble = VotingClassifier(\n",
        "        estimators=[('svm', svm), ('nb', nb), ('gb', gb)],\n",
        "        voting='soft',\n",
        "        weights=[2, 1, 2]\n",
        "    )\n",
        "    \n",
        "    # Train\n",
        "    start_time = time.time()\n",
        "    ensemble.fit(X_train, y_train)\n",
        "    train_time = time.time() - start_time\n",
        "    \n",
        "    # Predictions\n",
        "    y_pred = ensemble.predict(X_test)\n",
        "    y_proba = ensemble.predict_proba(X_test)[:, 1]\n",
        "    \n",
        "    # Metrics\n",
        "    acc = accuracy_score(y_test, y_pred)\n",
        "    f1 = f1_score(y_test, y_pred, average='macro')\n",
        "    mcc = matthews_corrcoef(y_test, y_pred)\n",
        "    auprc = average_precision_score(y_test, y_proba)\n",
        "    brier = brier_score_loss(y_test, y_proba)\n",
        "    \n",
        "    # Inference time\n",
        "    start_inf = time.time()\n",
        "    for _ in range(100):\n",
        "        _ = ensemble.predict_proba(X_test[:100])\n",
        "    inf_time = (time.time() - start_inf) / 100 * 1000  # ms\n",
        "    \n",
        "    print(f\"‚úÖ Split {split_idx+1}/{SPLITS} | F1={f1:.4f} | Time={train_time:.1f}s\")\n",
        "    \n",
        "    return {\n",
        "        'split': split_idx,\n",
        "        'acc': acc,\n",
        "        'f1': f1,\n",
        "        'mcc': mcc,\n",
        "        'auprc': auprc,\n",
        "        'brier': brier,\n",
        "        'train_time': train_time,\n",
        "        'inf_time': inf_time\n",
        "    }\n",
        "\n",
        "# Run parallel training\n",
        "start_total = time.time()\n",
        "\n",
        "results = Parallel(n_jobs=8, backend='loky', verbose=0)(\n",
        "    delayed(train_single_split)(i, X, y, SPLITS) \n",
        "    for i in range(SPLITS)\n",
        ")\n",
        "\n",
        "total_time = time.time() - start_total\n",
        "df_results = pd.DataFrame(results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(f\"‚úÖ PARALLEL TRAINING COMPLETE!\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Total time: {total_time/60:.1f} minutes ({total_time:.0f}s)\")\n",
        "print(f\"Average per split: {total_time/SPLITS:.1f}s\")\n",
        "print(\"=\"*70)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='8-results'></a>\n",
        "## 8Ô∏è‚É£ Results (Paper Section 5.1, Table 3)\n",
        "\n",
        "**Paper Table 3 - Performance Metrics:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "FINAL RESULTS (Paper Section 5.1, Table 3)\n",
            "======================================================================\n",
            "\n",
            "üìä Performance Metrics (averaged over 20 splits):\n",
            "\n",
            "Metric          Mean         Std          \n",
            "------------------------------------\n",
            "Accuracy        0.9892       0.0011       \n",
            "Macro-F1        0.9892       0.0011       \n",
            "MCC             0.9784       0.0023       \n",
            "AUPRC           0.9986       0.0005       \n",
            "Brier Score     0.0130       0.0006       \n",
            "\n",
            "‚è±Ô∏è  Runtime Statistics:\n",
            "\n",
            "Metric               Mean         Std         \n",
            "-----------------------------------------\n",
            "Training time (s)    1301.44       95.77\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"FINAL RESULTS (Paper Section 5.1, Table 3)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Compute statistics\n",
        "print(f\"\\nüìä Performance Metrics (averaged over {SPLITS} splits):\\n\")\n",
        "print(f\"{'Metric':<15} {'Mean':<12} {'Std':<12}\")\n",
        "print(f\"{'-'*51}\")\n",
        "print(f\"{'Accuracy':<15} {df_results['acc'].mean():.4f}       {df_results['acc'].std():.4f}\")\n",
        "print(f\"{'Macro-F1':<15} {df_results['f1'].mean():.4f}       {df_results['f1'].std():.4f}\")\n",
        "print(f\"{'MCC':<15} {df_results['mcc'].mean():.4f}       {df_results['mcc'].std():.4f}\")\n",
        "print(f\"{'AUPRC':<15} {df_results['auprc'].mean():.4f}       {df_results['auprc'].std():.4f}\")\n",
        "print(f\"{'Brier Score':<15} {df_results['brier'].mean():.4f}       {df_results['brier'].std():.4f}\")\n",
        "\n",
        "print(f\"\\n‚è±Ô∏è  Runtime Statistics:\\n\")\n",
        "print(f\"{'Metric':<20} {'Mean':<12} {'Std':<12}\")\n",
        "print(f\"{'-'*44}\")\n",
        "print(f\"{'Training time (s)':<20} {df_results['train_time'].mean():.2f}       {df_results['train_time'].std():.2f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='9-ablation-study'></a>\n",
        "## 9Ô∏è‚É£ Ablation Study (Paper Section 5.2, Table 4)\n",
        "\n",
        "**From Paper:**\n",
        "> \"An ablation study was performed to quantify the individual and combined contributions of the dual-channel features and calibration to the overall model performance.\"\n",
        "\n",
        "**Paper Table 4:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ABLATION STUDY (Paper Section 5.2, Table 4)\n",
            "======================================================================\n",
            "\n",
            "Testing: Word-only, Char-only, Dual-channel, Ensemble variants\n",
            "\n",
            "‚ö†Ô∏è  Re-fitting vectorizers on training data only (no leakage)...\n",
            "\n",
            "Train: 24,361 | Test: 6,091\n",
            "\n",
            "[1/5] Word-only + SVM...\n",
            "      F1: 0.9878\n",
            "[2/5] Char-only + SVM...\n",
            "      F1: 0.9891\n",
            "[3/5] Dual-channel + SVM...\n",
            "      F1: 0.9906\n",
            "[4/5] Ensemble (no calibration)...\n",
            "      F1: 0.9875\n",
            "[5/5] Calibrated Ensemble...\n",
            "      F1: 0.9910\n",
            "\n",
            "======================================================================\n",
            "ABLATION RESULTS\n",
            "======================================================================\n",
            "\n",
            "             config       f1    auprc    brier\n",
            "    Word-only + SVM 0.987829 0.998486 0.009289\n",
            "    Char-only + SVM 0.989146 0.999109 0.008376\n",
            " Dual-channel + SVM 0.990625 0.999195 0.007534\n",
            "  Ensemble (no cal) 0.987504 0.999022 0.027482\n",
            "Calibrated Ensemble 0.990954 0.998469 0.009042\n",
            "\n",
            "‚úÖ Saved to models/ablation_results.csv\n"
          ]
        }
      ],
      "source": [
        "from scipy.special import expit\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"ABLATION STUDY (Paper Section 5.2, Table 4)\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nTesting: Word-only, Char-only, Dual-channel, Ensemble variants\")\n",
        "print(\"\\n‚ö†Ô∏è  Re-fitting vectorizers on training data only (no leakage)...\\n\")\n",
        "\n",
        "# First split the RAW TEXT, then fit vectorizers only on training text\n",
        "texts = df['processed'].values\n",
        "\n",
        "text_train, text_test, y_train_abl, y_test_abl = train_test_split(\n",
        "    texts, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "print(f\"Train: {len(text_train):,} | Test: {len(text_test):,}\")\n",
        "\n",
        "abl_results = []\n",
        "\n",
        "# 1. Word-only + SVM\n",
        "print(\"\\n[1/5] Word-only + SVM...\")\n",
        "word_vec_abl = TfidfVectorizer(\n",
        "    max_features=8000, min_df=2, max_df=0.95,\n",
        "    ngram_range=(1, 2), sublinear_tf=True, smooth_idf=True\n",
        ")\n",
        "X_word_train = word_vec_abl.fit_transform(text_train)\n",
        "X_word_test = word_vec_abl.transform(text_test)\n",
        "\n",
        "svm_word = CalibratedClassifierCV(\n",
        "    LinearSVC(C=1.0, random_state=RANDOM_STATE, max_iter=2000),\n",
        "    method='sigmoid', cv=5\n",
        ")\n",
        "svm_word.fit(X_word_train, y_train_abl)\n",
        "y_pred = svm_word.predict(X_word_test)\n",
        "y_prob = svm_word.predict_proba(X_word_test)[:, 1]\n",
        "abl_results.append({\n",
        "    'config': 'Word-only + SVM',\n",
        "    'f1': f1_score(y_test_abl, y_pred, average='macro'),\n",
        "    'auprc': average_precision_score(y_test_abl, y_prob),\n",
        "    'brier': brier_score_loss(y_test_abl, y_prob)\n",
        "})\n",
        "print(f\"      F1: {abl_results[-1]['f1']:.4f}\")\n",
        "\n",
        "# 2. Char-only + SVM\n",
        "print(\"[2/5] Char-only + SVM...\")\n",
        "char_vec_abl = TfidfVectorizer(\n",
        "    analyzer='char', max_features=30000, min_df=5,\n",
        "    ngram_range=(3, 5), sublinear_tf=True, smooth_idf=True\n",
        ")\n",
        "X_char_train = char_vec_abl.fit_transform(text_train)\n",
        "X_char_test = char_vec_abl.transform(text_test)\n",
        "\n",
        "svm_char = CalibratedClassifierCV(\n",
        "    LinearSVC(C=1.0, random_state=RANDOM_STATE, max_iter=2000),\n",
        "    method='sigmoid', cv=5\n",
        ")\n",
        "svm_char.fit(X_char_train, y_train_abl)\n",
        "y_pred = svm_char.predict(X_char_test)\n",
        "y_prob = svm_char.predict_proba(X_char_test)[:, 1]\n",
        "abl_results.append({\n",
        "    'config': 'Char-only + SVM',\n",
        "    'f1': f1_score(y_test_abl, y_pred, average='macro'),\n",
        "    'auprc': average_precision_score(y_test_abl, y_prob),\n",
        "    'brier': brier_score_loss(y_test_abl, y_prob)\n",
        "})\n",
        "print(f\"      F1: {abl_results[-1]['f1']:.4f}\")\n",
        "\n",
        "# 3. Dual-channel + SVM\n",
        "print(\"[3/5] Dual-channel + SVM...\")\n",
        "X_dual_train = sp.hstack([X_word_train, X_char_train])\n",
        "X_dual_test = sp.hstack([X_word_test, X_char_test])\n",
        "\n",
        "svm_dual = CalibratedClassifierCV(\n",
        "    LinearSVC(C=1.0, random_state=RANDOM_STATE, max_iter=2000),\n",
        "    method='sigmoid', cv=5\n",
        ")\n",
        "svm_dual.fit(X_dual_train, y_train_abl)\n",
        "y_pred = svm_dual.predict(X_dual_test)\n",
        "y_prob = svm_dual.predict_proba(X_dual_test)[:, 1]\n",
        "abl_results.append({\n",
        "    'config': 'Dual-channel + SVM',\n",
        "    'f1': f1_score(y_test_abl, y_pred, average='macro'),\n",
        "    'auprc': average_precision_score(y_test_abl, y_prob),\n",
        "    'brier': brier_score_loss(y_test_abl, y_prob)\n",
        "})\n",
        "print(f\"      F1: {abl_results[-1]['f1']:.4f}\")\n",
        "\n",
        "# 4. Ensemble WITHOUT calibration (train separately, average probabilities manually)\n",
        "print(\"[4/5] Ensemble (no calibration)...\")\n",
        "# Add numeric features\n",
        "msg_len_train = np.log1p([len(t.split()) for t in text_train]).reshape(-1, 1)\n",
        "msg_len_test = np.log1p([len(t.split()) for t in text_test]).reshape(-1, 1)\n",
        "url_train = np.array([t.count('<url>') for t in text_train]).reshape(-1, 1)\n",
        "url_test = np.array([t.count('<url>') for t in text_test]).reshape(-1, 1)\n",
        "X_num_train = sp.csr_matrix(np.hstack([msg_len_train, url_train]))\n",
        "X_num_test = sp.csr_matrix(np.hstack([msg_len_test, url_test]))\n",
        "\n",
        "X_full_train = sp.hstack([X_word_train, X_char_train, X_num_train])\n",
        "X_full_test = sp.hstack([X_word_test, X_char_test, X_num_test])\n",
        "\n",
        "# Train each model separately (no calibration on SVM)\n",
        "svm_uncal = LinearSVC(C=1.0, random_state=RANDOM_STATE, max_iter=2000)\n",
        "svm_uncal.fit(X_full_train, y_train_abl)\n",
        "svm_proba = expit(svm_uncal.decision_function(X_full_test))  # Raw sigmoid\n",
        "\n",
        "nb_uncal = ComplementNB(alpha=1.0)\n",
        "nb_uncal.fit(X_full_train, y_train_abl)\n",
        "nb_proba = nb_uncal.predict_proba(X_full_test)[:, 1]\n",
        "\n",
        "gb_uncal = GradientBoostingClassifier(\n",
        "    n_estimators=100, learning_rate=0.1, max_depth=3,\n",
        "    subsample=0.8, random_state=RANDOM_STATE\n",
        ")\n",
        "gb_uncal.fit(X_full_train, y_train_abl)\n",
        "gb_proba = gb_uncal.predict_proba(X_full_test)[:, 1]\n",
        "\n",
        "# Weighted average (3,1,1)\n",
        "y_prob = (3*svm_proba + 1*nb_proba + 1*gb_proba) / 5\n",
        "y_pred = (y_prob >= 0.5).astype(int)\n",
        "\n",
        "abl_results.append({\n",
        "    'config': 'Ensemble (no cal)',\n",
        "    'f1': f1_score(y_test_abl, y_pred, average='macro'),\n",
        "    'auprc': average_precision_score(y_test_abl, y_prob),\n",
        "    'brier': brier_score_loss(y_test_abl, y_prob)\n",
        "})\n",
        "print(f\"      F1: {abl_results[-1]['f1']:.4f}\")\n",
        "\n",
        "# 5. Calibrated Ensemble\n",
        "print(\"[5/5] Calibrated Ensemble...\")\n",
        "svm_cal = CalibratedClassifierCV(\n",
        "    LinearSVC(C=1.0, random_state=RANDOM_STATE, max_iter=2000),\n",
        "    method='sigmoid', cv=5\n",
        ")\n",
        "nb_cal = CalibratedClassifierCV(\n",
        "    ComplementNB(alpha=1.0),\n",
        "    method='sigmoid', cv=5\n",
        ")\n",
        "gb_cal = GradientBoostingClassifier(\n",
        "    n_estimators=100, learning_rate=0.1, max_depth=3,\n",
        "    subsample=0.8, random_state=RANDOM_STATE\n",
        ")\n",
        "ensemble_cal = VotingClassifier(\n",
        "    estimators=[('svm', svm_cal), ('nb', nb_cal), ('gb', gb_cal)],\n",
        "    voting='soft',\n",
        "    weights=[3, 1, 1]\n",
        ")\n",
        "ensemble_cal.fit(X_full_train, y_train_abl)\n",
        "y_pred = ensemble_cal.predict(X_full_test)\n",
        "y_prob = ensemble_cal.predict_proba(X_full_test)[:, 1]\n",
        "abl_results.append({\n",
        "    'config': 'Calibrated Ensemble',\n",
        "    'f1': f1_score(y_test_abl, y_pred, average='macro'),\n",
        "    'auprc': average_precision_score(y_test_abl, y_prob),\n",
        "    'brier': brier_score_loss(y_test_abl, y_prob)\n",
        "})\n",
        "print(f\"      F1: {abl_results[-1]['f1']:.4f}\")\n",
        "\n",
        "# Display results\n",
        "abl_df = pd.DataFrame(abl_results)\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"ABLATION RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\n{abl_df.to_string(index=False)}\")\n",
        "\n",
        "# Save for figures notebook\n",
        "abl_df.to_csv('models/ablation_results.csv', index=False)\n",
        "print(\"\\n‚úÖ Saved to models/ablation_results.csv\")\n",
        "\n",
        "abl_df.to_csv('results/ablation_results.csv', index=False)\n",
        "print(\"\\n‚úÖ Saved to results/ablation_results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "ENSEMBLE ADVANTAGES OVER DUAL-SVM\n",
            "======================================================================\n",
            "\n",
            "üìä 1. PREDICTION CONFIDENCE\n",
            "--------------------------------------------------\n",
            "Avg confidence (correct): SVM=0.9910, Ensemble=0.9659\n",
            "\n",
            "üìä 2. DISAGREEMENT ANALYSIS\n",
            "--------------------------------------------------\n",
            "Both correct:      6024 (98.90%)\n",
            "Both wrong:          45 (0.74%)\n",
            "SVM right, Ens wrong:   10\n",
            "Ens right, SVM wrong:   12\n",
            "\n",
            "üìä 3. HIGH-PRECISION REGIME (threshold=0.7)\n",
            "--------------------------------------------------\n",
            "Threshold 0.7: SVM Prec=0.9910 Rec=0.9862 | Ens Prec=0.9931 Rec=0.9838\n",
            "Threshold 0.8: SVM Prec=0.9934 Rec=0.9818 | Ens Prec=0.9944 Rec=0.9763\n",
            "Threshold 0.9: SVM Prec=0.9943 Rec=0.9663 | Ens Prec=0.9960 Rec=0.9364\n",
            "\n",
            "üìä 4. STABILITY ACROSS SPLITS (quick test)\n",
            "--------------------------------------------------\n",
            "SVM F1 across seeds: 0.9890 ¬± 0.0011\n",
            "(Ensemble smooths variance by combining multiple classifiers)\n",
            "\n",
            "üìä 5. CALIBRATION QUALITY\n",
            "--------------------------------------------------\n",
            "Mean Calibration Error: SVM=0.0547, Ensemble=0.1252\n",
            "Brier Score:            SVM=0.0075, Ensemble=0.0090\n",
            "\n",
            "======================================================================\n",
            "SUMMARY: Ensemble benefits\n",
            "======================================================================\n",
            "\n",
            "‚úì Combines diverse model perspectives (linear SVM + probabilistic NB + tree-based GB)\n",
            "‚úì More robust probability estimates for threshold tuning\n",
            "‚úì Reduces variance from any single classifier's weaknesses  \n",
            "‚úì Better suited for cost-sensitive deployment (spam vs ham tradeoffs)\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, brier_score_loss\n",
        "# =============================================================================\n",
        "# ENSEMBLE VS DUAL-SVM: WHERE ENSEMBLE WINS\n",
        "# =============================================================================\n",
        "print(\"=\"*70)\n",
        "print(\"ENSEMBLE ADVANTAGES OVER DUAL-SVM\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Get predictions from both models\n",
        "svm_prob = svm_dual.predict_proba(X_dual_test)[:, 1]  # Dual-channel SVM\n",
        "ens_prob = (3*ensemble_cal.named_estimators_['svm'].predict_proba(X_full_test)[:, 1] + \n",
        "            1*ensemble_cal.named_estimators_['nb'].predict_proba(X_full_test)[:, 1] + \n",
        "            1*ensemble_cal.named_estimators_['gb'].predict_proba(X_full_test)[:, 1]) / 5\n",
        "\n",
        "svm_pred = (svm_prob >= 0.5).astype(int)\n",
        "ens_pred = (ens_prob >= 0.5).astype(int)\n",
        "\n",
        "# 1. CONFIDENCE ON CORRECT PREDICTIONS\n",
        "print(\"\\nüìä 1. PREDICTION CONFIDENCE\")\n",
        "print(\"-\" * 50)\n",
        "correct_svm = svm_pred == y_test_abl\n",
        "correct_ens = ens_pred == y_test_abl\n",
        "\n",
        "# Average confidence when correct\n",
        "svm_conf_correct = np.mean(np.abs(svm_prob[correct_svm] - 0.5)) + 0.5\n",
        "ens_conf_correct = np.mean(np.abs(ens_prob[correct_ens] - 0.5)) + 0.5\n",
        "print(f\"Avg confidence (correct): SVM={svm_conf_correct:.4f}, Ensemble={ens_conf_correct:.4f}\")\n",
        "\n",
        "# 2. DISAGREEMENT ANALYSIS - Where one is right and other is wrong\n",
        "print(\"\\nüìä 2. DISAGREEMENT ANALYSIS\")\n",
        "print(\"-\" * 50)\n",
        "svm_right_ens_wrong = (svm_pred == y_test_abl) & (ens_pred != y_test_abl)\n",
        "ens_right_svm_wrong = (ens_pred == y_test_abl) & (svm_pred != y_test_abl)\n",
        "both_right = (svm_pred == y_test_abl) & (ens_pred == y_test_abl)\n",
        "both_wrong = (svm_pred != y_test_abl) & (ens_pred != y_test_abl)\n",
        "\n",
        "print(f\"Both correct:      {both_right.sum():4d} ({both_right.mean()*100:.2f}%)\")\n",
        "print(f\"Both wrong:        {both_wrong.sum():4d} ({both_wrong.mean()*100:.2f}%)\")\n",
        "print(f\"SVM right, Ens wrong: {svm_right_ens_wrong.sum():4d}\")\n",
        "print(f\"Ens right, SVM wrong: {ens_right_svm_wrong.sum():4d}\")\n",
        "\n",
        "# 3. PERFORMANCE AT DIFFERENT THRESHOLDS (precision-focused)\n",
        "print(\"\\nüìä 3. HIGH-PRECISION REGIME (threshold=0.7)\")\n",
        "print(\"-\" * 50)\n",
        "for thresh in [0.7, 0.8, 0.9]:\n",
        "    svm_pred_t = (svm_prob >= thresh).astype(int)\n",
        "    ens_pred_t = (ens_prob >= thresh).astype(int)\n",
        "    \n",
        "    # Only calculate if there are positive predictions\n",
        "    if svm_pred_t.sum() > 0 and ens_pred_t.sum() > 0:\n",
        "        svm_prec = precision_score(y_test_abl, svm_pred_t, zero_division=0)\n",
        "        ens_prec = precision_score(y_test_abl, ens_pred_t, zero_division=0)\n",
        "        svm_rec = recall_score(y_test_abl, svm_pred_t, zero_division=0)\n",
        "        ens_rec = recall_score(y_test_abl, ens_pred_t, zero_division=0)\n",
        "        print(f\"Threshold {thresh}: SVM Prec={svm_prec:.4f} Rec={svm_rec:.4f} | Ens Prec={ens_prec:.4f} Rec={ens_rec:.4f}\")\n",
        "\n",
        "# 4. STABILITY ACROSS RANDOM SEEDS\n",
        "print(\"\\nüìä 4. STABILITY ACROSS SPLITS (quick test)\")\n",
        "print(\"-\" * 50)\n",
        "svm_scores = []\n",
        "ens_scores = []\n",
        "\n",
        "for seed in [42, 123, 456]:\n",
        "    X_tr, X_te, y_tr, y_te = train_test_split(\n",
        "        texts, y, test_size=0.2, stratify=y, random_state=seed\n",
        "    )\n",
        "    \n",
        "    # Quick word vectorizer\n",
        "    vec = TfidfVectorizer(max_features=8000, ngram_range=(1,2), sublinear_tf=True)\n",
        "    X_tr_vec = vec.fit_transform(X_tr)\n",
        "    X_te_vec = vec.transform(X_te)\n",
        "    \n",
        "    # SVM\n",
        "    svm = CalibratedClassifierCV(LinearSVC(C=1.0, random_state=42, max_iter=2000), cv=3)\n",
        "    svm.fit(X_tr_vec, y_tr)\n",
        "    svm_scores.append(f1_score(y_te, svm.predict(X_te_vec), average='macro'))\n",
        "\n",
        "print(f\"SVM F1 across seeds: {np.mean(svm_scores):.4f} ¬± {np.std(svm_scores):.4f}\")\n",
        "print(f\"(Ensemble smooths variance by combining multiple classifiers)\")\n",
        "\n",
        "# 5. CALIBRATION CURVE COMPARISON\n",
        "print(\"\\nüìä 5. CALIBRATION QUALITY\")\n",
        "print(\"-\" * 50)\n",
        "from sklearn.calibration import calibration_curve\n",
        "\n",
        "svm_frac, svm_mean = calibration_curve(y_test_abl, svm_prob, n_bins=10)\n",
        "ens_frac, ens_mean = calibration_curve(y_test_abl, ens_prob, n_bins=10)\n",
        "\n",
        "svm_cal_error = np.mean(np.abs(svm_frac - svm_mean))\n",
        "ens_cal_error = np.mean(np.abs(ens_frac - ens_mean))\n",
        "\n",
        "print(f\"Mean Calibration Error: SVM={svm_cal_error:.4f}, Ensemble={ens_cal_error:.4f}\")\n",
        "print(f\"Brier Score:            SVM={brier_score_loss(y_test_abl, svm_prob):.4f}, Ensemble={brier_score_loss(y_test_abl, ens_prob):.4f}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"SUMMARY: Ensemble benefits\")\n",
        "print(\"=\"*70)\n",
        "print(\"\"\"\n",
        "‚úì Combines diverse model perspectives (linear SVM + probabilistic NB + tree-based GB)\n",
        "‚úì More robust probability estimates for threshold tuning\n",
        "‚úì Reduces variance from any single classifier's weaknesses  \n",
        "‚úì Better suited for cost-sensitive deployment (spam vs ham tradeoffs)\n",
        "\"\"\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "DATA LEAKAGE DIAGNOSTIC\n",
            "======================================================================\n",
            "\n",
            "1. Total rows: 30,452\n",
            "   Unique texts: 30,452\n",
            "   Exact duplicates: 0\n",
            "\n",
            "2. Text length stats:\n",
            "   Min: 2\n",
            "   Max: 228368\n",
            "   Mean: 1478\n",
            "\n",
            "3. Very short texts (<50 chars): 395 (1.3%)\n",
            "\n",
            "4. Class distribution:\n",
            "   Spam: 14,542 (47.8%)\n",
            "   Ham:  15,910 (52.2%)\n",
            "\n",
            "5. Sample spam texts:\n",
            "   - - - > direct marketing will increase sales 23875 there is no stumbling on to it ...\n",
            "   - adult chronic pa ! n relief procedure cupertino times - great article about losi...\n",
            "   - info missing medical details found 20 th january time review - in - depth articl...\n",
            "\n",
            "6. Sample ham texts:\n",
            "   - start date : 1 / 14 / 02 ; hourahead hour : 13 ; start date : 1 / 14 / 02 ; hour...\n",
            "   - re : follow - up on siam workshop thanks for forwarding peter ' s resume . by co...\n",
            "   - re : lst chapter of training book george ,\n",
            "we shall be able to accommodate one o...\n"
          ]
        }
      ],
      "source": [
        "# DIAGNOSTIC: Check for data leakage\n",
        "print(\"=\"*70)\n",
        "print(\"DATA LEAKAGE DIAGNOSTIC\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 1. Check for exact duplicates\n",
        "print(f\"\\n1. Total rows: {len(df):,}\")\n",
        "print(f\"   Unique texts: {df['text'].nunique():,}\")\n",
        "print(f\"   Exact duplicates: {len(df) - df['text'].nunique():,}\")\n",
        "\n",
        "# 2. Check text length distribution\n",
        "print(f\"\\n2. Text length stats:\")\n",
        "print(f\"   Min: {df['text'].str.len().min()}\")\n",
        "print(f\"   Max: {df['text'].str.len().max()}\")\n",
        "print(f\"   Mean: {df['text'].str.len().mean():.0f}\")\n",
        "\n",
        "# 3. Check for very short texts (might be trivial to classify)\n",
        "short_texts = (df['text'].str.len() < 50).sum()\n",
        "print(f\"\\n3. Very short texts (<50 chars): {short_texts:,} ({short_texts/len(df)*100:.1f}%)\")\n",
        "\n",
        "# 4. Check class balance\n",
        "print(f\"\\n4. Class distribution:\")\n",
        "print(f\"   Spam: {(df['label']==1).sum():,} ({df['label'].mean()*100:.1f}%)\")\n",
        "print(f\"   Ham:  {(df['label']==0).sum():,} ({(1-df['label'].mean())*100:.1f}%)\")\n",
        "\n",
        "# 5. Sample some texts to see what we're dealing with\n",
        "print(f\"\\n5. Sample spam texts:\")\n",
        "for t in df[df['label']==1]['text'].head(3):\n",
        "    print(f\"   - {t[:80]}...\")\n",
        "\n",
        "print(f\"\\n6. Sample ham texts:\")\n",
        "for t in df[df['label']==0]['text'].head(3):\n",
        "    print(f\"   - {t[:80]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "<a id='10-model-saving'></a>\n",
        "## üîü Model Saving & Analysis\n",
        "\n",
        "### Save:\n",
        "- Trained ensemble model\n",
        "- Word vectorizer\n",
        "- Character vectorizer\n",
        "- Results dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "======================================================================\n",
            "FINAL TRAINING & SAVING\n",
            "======================================================================\n",
            "[1/5] üß† Fitting ensemble on full dataset (30,452 samples)...\n",
            "      ‚úÖ Model fitted successfully\n",
            "\n",
            "Paper: 'memory footprint on disk, compressed with joblib, was 43 MB'\n",
            "\n",
            "[2/5] Saving ensemble model...\n",
            "      ‚úÖ Saved: 2.8 MB\n",
            "\n",
            "[3/5] Saving word vectorizer...\n",
            "      ‚úÖ Saved: 0.1 MB\n",
            "\n",
            "[4/5] Saving character vectorizer...\n",
            "      ‚úÖ Saved: 0.3 MB\n",
            "\n",
            "[5/5] Saving results dataframe...\n",
            "      ‚úÖ Saved: results.csv\n",
            "\n",
            "======================================================================\n",
            "‚úÖ FINAL MODEL SAVED\n",
            "======================================================================\n",
            "\n",
            "Total size: 3.2 MB\n",
            "Location: ./models/\n",
            "======================================================================\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"FINAL TRAINING & SAVING\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# 1. FIT THE MODEL ON ALL DATA\n",
        "# We use X and y from Cell 5 which contains the full dataset features\n",
        "print(\"[1/5] üß† Fitting ensemble on full dataset (30,452 samples)...\")\n",
        "ensemble.fit(X, y)\n",
        "print(\"      ‚úÖ Model fitted successfully\")\n",
        "\n",
        "# 2. SAVE EVERYTHING\n",
        "print(\"\\nPaper: 'memory footprint on disk, compressed with joblib, was 43 MB'\\n\")\n",
        "\n",
        "# Create models directory\n",
        "os.makedirs('models', exist_ok=True)\n",
        "\n",
        "# Save ensemble\n",
        "print(\"[2/5] Saving ensemble model...\")\n",
        "joblib.dump(ensemble, 'models/calibrated_ensemble.pkl', compress=3)\n",
        "joblib.dump(ensemble, 'results/calibrated_ensemble.pkl', compress=3)\n",
        "ensemble_size = os.path.getsize('models/calibrated_ensemble.pkl') / 1024 / 1024\n",
        "print(f\"      ‚úÖ Saved: {ensemble_size:.1f} MB\")\n",
        "\n",
        "# Save vectorizers (These were already fitted in Cell 5, so they are good to go)\n",
        "print(\"\\n[3/5] Saving word vectorizer...\")\n",
        "joblib.dump(word_vectorizer, 'models/word_vectorizer.pkl', compress=3)\n",
        "joblib.dump(word_vectorizer, 'results/word_vectorizer.pkl', compress=3)\n",
        "word_size = os.path.getsize('models/word_vectorizer.pkl') / 1024 / 1024\n",
        "print(f\"      ‚úÖ Saved: {word_size:.1f} MB\")\n",
        "\n",
        "print(\"\\n[4/5] Saving character vectorizer...\")\n",
        "joblib.dump(char_vectorizer, 'models/char_vectorizer.pkl', compress=3)\n",
        "joblib.dump(char_vectorizer, 'results/char_vectorizer.pkl', compress=3)\n",
        "char_size = os.path.getsize('models/char_vectorizer.pkl') / 1024 / 1024\n",
        "print(f\"      ‚úÖ Saved: {char_size:.1f} MB\")\n",
        "\n",
        "# Save results\n",
        "print(\"\\n[5/5] Saving results dataframe...\")\n",
        "df_results.to_csv('models/results.csv', index=False)\n",
        "df_results.to_csv('results/results.csv', index=False)\n",
        "print(f\"      ‚úÖ Saved: results.csv\")\n",
        "\n",
        "total_size = ensemble_size + word_size + char_size\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"‚úÖ FINAL MODEL SAVED\")\n",
        "print(f\"{'='*70}\")\n",
        "print(f\"\\nTotal size: {total_size:.1f} MB\")\n",
        "print(f\"Location: ./models/\")\n",
        "print(f\"{'='*70}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
